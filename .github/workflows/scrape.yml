name: Automated RFP Scraping

on:
  # Scheduled runs every 6 hours (configurable via repository settings)
  schedule:
    - cron: '0 */6 * * *'  # Every 6 hours at minute 0
  
  # Manual trigger for force updates
  workflow_dispatch:
    inputs:
      force_full_scrape:
        description: 'Force full scrape of all sites'
        required: false
        default: false
        type: boolean
      target_sites:
        description: 'Comma-separated site IDs to scrape (leave empty for all)'
        required: false
        default: ''
        type: string

  # Also run on pushes to main for testing and when sites are added
  push:
    branches: [ main ]
    paths: 
      - 'backend/**'
      - '.github/workflows/scrape.yml'
      - 'data/sites.json'

env:
  PYTHON_VERSION: '3.10'
  NODE_VERSION: '18'

jobs:
  scrape-rfps:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        # Fetch full history for proper data archiving
        fetch-depth: 0
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Cache Python dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('backend/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install Python dependencies
      run: |
        cd backend
        pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Install Playwright browsers
      run: |
        cd backend
        # Install browsers without system dependencies first
        playwright install chromium
        
        # Install minimal system dependencies manually if needed
        sudo apt-get update
        sudo apt-get install -y --no-install-recommends \
          libnss3 \
          libnspr4 \
          libatk-bridge2.0-0 \
          libdrm2 \
          libxkbcommon0 \
          libgtk-3-0 \
          libgbm1 \
          libasound2-dev || sudo apt-get install -y --no-install-recommends libasound2t64

    - name: Configure Git for automated commits
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "RFP Monitor Bot"

    - name: Check last scrape time and cadence settings
      id: scrape_check
      run: |
        # Check if we should run based on cadence settings
        SHOULD_SCRAPE="true"
        SCRAPE_REASON="scheduled"
        
        # Check for force update flag
        if [ "${{ github.event.inputs.force_full_scrape }}" = "true" ]; then
          SHOULD_SCRAPE="true"
          SCRAPE_REASON="force_update"
          echo "Force update requested via workflow dispatch"
        fi
        
        # Check if this is a manual dispatch
        if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          SHOULD_SCRAPE="true"
          SCRAPE_REASON="manual_trigger"
          echo "Manual scrape triggered"
        fi
        
        # Check if this is triggered by sites.json update (new site added)
        if [ "${{ github.event_name }}" = "push" ] && echo "${{ github.event.head_commit.modified }}" | grep -q "data/sites.json"; then
          SHOULD_SCRAPE="true"
          SCRAPE_REASON="new_site_added"
          echo "New site detected - triggering scrape"
        fi
        
        # Check last scrape time (if file exists)
        if [ -f "data/last_scrape.txt" ]; then
          LAST_SCRAPE=$(cat data/last_scrape.txt)
          echo "Last scrape: $LAST_SCRAPE"
          
          # Calculate time since last scrape (basic check - could be enhanced)
          CURRENT_TIME=$(date +%s)
          LAST_SCRAPE_TIME=$(date -d "$LAST_SCRAPE" +%s 2>/dev/null || echo "0")
          TIME_DIFF=$((CURRENT_TIME - LAST_SCRAPE_TIME))
          HOURS_SINCE=$((TIME_DIFF / 3600))
          
          echo "Hours since last scrape: $HOURS_SINCE"
          
          # Default cadence is 6 hours - could be made configurable via repository variables
          MIN_HOURS_BETWEEN_SCRAPES=${MIN_SCRAPE_INTERVAL_HOURS:-6}
          
          if [ $HOURS_SINCE -lt $MIN_HOURS_BETWEEN_SCRAPES ] && [ "$SCRAPE_REASON" != "force_update" ] && [ "$SCRAPE_REASON" != "manual_trigger" ] && [ "$SCRAPE_REASON" != "new_site_added" ]; then
            SHOULD_SCRAPE="false"
            echo "Skipping scrape - too soon since last run ($HOURS_SINCE < $MIN_HOURS_BETWEEN_SCRAPES hours)"
          fi
        else
          echo "No previous scrape record found - proceeding with scrape"
        fi
        
        echo "should_scrape=$SHOULD_SCRAPE" >> $GITHUB_OUTPUT
        echo "scrape_reason=$SCRAPE_REASON" >> $GITHUB_OUTPUT

    - name: Run RFP scraping
      if: steps.scrape_check.outputs.should_scrape == 'true'
      id: scrape
      run: |
        cd backend
        echo "ðŸ•¸ï¸ Starting RFP scraping process..."
        echo "Reason: ${{ steps.scrape_check.outputs.scrape_reason }}"
        
        # Set target sites if specified
        TARGET_SITES_ARG=""
        if [ -n "${{ github.event.inputs.target_sites }}" ]; then
          TARGET_SITES_ARG="--sites ${{ github.event.inputs.target_sites }}"
          echo "Targeting specific sites: ${{ github.event.inputs.target_sites }}"
        fi
        
        # Run the scraper with appropriate flags
        FORCE_FLAG=""
        if [ "${{ github.event.inputs.force_full_scrape }}" = "true" ]; then
          FORCE_FLAG="--force"
        fi
        
        # Check if sites are configured
        if [ ! -f "../data/sites.json" ] || [ ! -s "../data/sites.json" ]; then
          echo "âš ï¸ No sites configured yet - this is normal for initial setup"
          echo "Creating empty data files for deployment..."
          mkdir -p ../data
          echo "[]" > ../data/rfps.json
          echo "[]" > ../data/sites.json
          echo "scrape_success=true" >> $GITHUB_OUTPUT
          echo "âœ… Initial setup completed (no sites to scrape yet)"
        else
          # Execute scraping with error handling
          echo "ðŸ•¸ï¸ Sites configured, attempting to scrape..."
          if python main.py scrape $TARGET_SITES_ARG $FORCE_FLAG --output-dir ../data; then
            echo "scrape_success=true" >> $GITHUB_OUTPUT
            echo "âœ… Scraping completed successfully"
          else
            echo "scrape_success=false" >> $GITHUB_OUTPUT
            echo "âŒ Scraping failed"
            exit 1
          fi
        fi
        
        # Update last scrape timestamp
        date -u +"%Y-%m-%dT%H:%M:%SZ" > ../data/last_scrape.txt
        
        # Count new/updated RFPs
        cd ..  # Go back to repo root
        NEW_RFPS_COUNT=$(python .github/scripts/count_rfps.py 2>/dev/null || echo "0")
        echo "rfp_count=$NEW_RFPS_COUNT" >> $GITHUB_OUTPUT
        cd backend  # Return to backend directory

    - name: Archive historical data
      if: steps.scrape_check.outputs.should_scrape == 'true' && steps.scrape.outputs.scrape_success == 'true'
      run: |
        # Create historical archive
        TIMESTAMP=$(date -u +"%Y%m%d_%H%M%S")
        mkdir -p data/history
        
        # Archive current data with timestamp
        if [ -f "data/rfps.json" ]; then
          cp data/rfps.json "data/history/rfps_$TIMESTAMP.json"
          echo "ðŸ“ Archived RFPs to data/history/rfps_$TIMESTAMP.json"
        fi
        
        # Keep only last 30 days of history to prevent repo bloat
        find data/history -name "rfps_*.json" -type f -mtime +30 -delete 2>/dev/null || true
        
        # Create summary statistics
        python .github/scripts/generate_stats.py

    - name: Commit and push changes
      if: steps.scrape_check.outputs.should_scrape == 'true' && steps.scrape.outputs.scrape_success == 'true'
      run: |
        # Check if there are any changes to commit
        if [ -n "$(git status --porcelain)" ]; then
          echo "ðŸ“ Changes detected, committing..."
          
          git add data/
          
          # Create detailed commit message
          COMMIT_MSG="ðŸ¤– Automated RFP update"
          if [ "${{ steps.scrape_check.outputs.scrape_reason }}" = "force_update" ]; then
            COMMIT_MSG="ðŸš€ Force update: RFP data refresh"
          elif [ "${{ steps.scrape_check.outputs.scrape_reason }}" = "manual_trigger" ]; then
            COMMIT_MSG="ðŸ‘¤ Manual trigger: RFP data update"
          fi
          
          # Create commit body with newlines
          printf -v COMMIT_BODY -- "- Scraped at: %s\n- Total RFPs: %s\n- Trigger: %s\n- Workflow: %s #%s" \
            "$(date -u +"%Y-%m-%d %H:%M:%S UTC")" \
            "${{ steps.scrape.outputs.rfp_count }}" \
            "${{ steps.scrape_check.outputs.scrape_reason }}" \
            "${{ github.workflow }}" \
            "${{ github.run_number }}"

          git commit -m "$COMMIT_MSG" -m "$COMMIT_BODY"
          git push
          
          echo "âœ… Changes committed and pushed to repository"
        else
          echo "â„¹ï¸ No changes detected - data is up to date"
        fi

    - name: Create scraping summary
      if: steps.scrape_check.outputs.should_scrape == 'true'
      run: |
        echo "## ðŸ•¸ï¸ RFP Scraping Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "- **Status**: ${{ steps.scrape.outputs.scrape_success == 'true' && 'âœ… Success' || 'âŒ Failed' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Trigger**: ${{ steps.scrape_check.outputs.scrape_reason }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Total RFPs**: ${{ steps.scrape.outputs.rfp_count || 'Unknown' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Timestamp**: $(date -u +"%Y-%m-%d %H:%M:%S UTC")" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ steps.scrape_check.outputs.should_scrape }}" != "true" ]; then
          echo "**Scrape skipped** - too soon since last update or other constraint" >> $GITHUB_STEP_SUMMARY
        fi

  # Trigger deployment after successful scrape
  trigger-deployment:
    needs: scrape-rfps
    if: needs.scrape-rfps.result == 'success'
    runs-on: ubuntu-latest
    
    steps:
    - name: Trigger deployment workflow
      uses: actions/github-script@v7
      with:
        script: |
          github.rest.actions.createWorkflowDispatch({
            owner: context.repo.owner,
            repo: context.repo.repo,
            workflow_id: 'deploy.yml',
            ref: 'main'
          });