name: Automated RFP Scraping

on:
  # Scheduled runs every 6 hours (configurable via repository settings)
  schedule:
    - cron: '0 */6 * * *'  # Every 6 hours at minute 0
  
  # Manual trigger for force updates
  workflow_dispatch:
    inputs:
      force_full_scrape:
        description: 'Force full scrape of all sites'
        required: false
        default: 'false'
        type: boolean
      target_sites:
        description: 'Comma-separated site IDs to scrape (leave empty for all)'
        required: false
        default: ''
        type: string

  # Also run on pushes to main for testing
  push:
    branches: [ main ]
    paths: 
      - 'backend/**'
      - '.github/workflows/scrape.yml'

env:
  PYTHON_VERSION: '3.10'
  NODE_VERSION: '18'

jobs:
  scrape-rfps:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        # Fetch full history for proper data archiving
        fetch-depth: 0
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Cache Python dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('backend/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install Python dependencies
      run: |
        cd backend
        pip install --upgrade pip
        pip install -r requirements.txt
        playwright install --with-deps chromium

    - name: Configure Git for automated commits
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "RFP Monitor Bot"

    - name: Check last scrape time and cadence settings
      id: scrape_check
      run: |
        # Check if we should run based on cadence settings
        SHOULD_SCRAPE="true"
        SCRAPE_REASON="scheduled"
        
        # Check for force update flag
        if [ "${{ github.event.inputs.force_full_scrape }}" = "true" ]; then
          SHOULD_SCRAPE="true"
          SCRAPE_REASON="force_update"
          echo "Force update requested via workflow dispatch"
        fi
        
        # Check if this is a manual dispatch
        if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          SHOULD_SCRAPE="true"
          SCRAPE_REASON="manual_trigger"
          echo "Manual scrape triggered"
        fi
        
        # Check last scrape time (if file exists)
        if [ -f "data/last_scrape.txt" ]; then
          LAST_SCRAPE=$(cat data/last_scrape.txt)
          echo "Last scrape: $LAST_SCRAPE"
          
          # Calculate time since last scrape (basic check - could be enhanced)
          CURRENT_TIME=$(date +%s)
          LAST_SCRAPE_TIME=$(date -d "$LAST_SCRAPE" +%s 2>/dev/null || echo "0")
          TIME_DIFF=$((CURRENT_TIME - LAST_SCRAPE_TIME))
          HOURS_SINCE=$((TIME_DIFF / 3600))
          
          echo "Hours since last scrape: $HOURS_SINCE"
          
          # Default cadence is 6 hours - could be made configurable via repository variables
          MIN_HOURS_BETWEEN_SCRAPES=${MIN_SCRAPE_INTERVAL_HOURS:-6}
          
          if [ $HOURS_SINCE -lt $MIN_HOURS_BETWEEN_SCRAPES ] && [ "$SCRAPE_REASON" != "force_update" ]; then
            SHOULD_SCRAPE="false"
            echo "Skipping scrape - too soon since last run ($HOURS_SINCE < $MIN_HOURS_BETWEEN_SCRAPES hours)"
          fi
        else
          echo "No previous scrape record found - proceeding with scrape"
        fi
        
        echo "should_scrape=$SHOULD_SCRAPE" >> $GITHUB_OUTPUT
        echo "scrape_reason=$SCRAPE_REASON" >> $GITHUB_OUTPUT

    - name: Run RFP scraping
      if: steps.scrape_check.outputs.should_scrape == 'true'
      id: scrape
      run: |
        cd backend
        echo "🕸️ Starting RFP scraping process..."
        echo "Reason: ${{ steps.scrape_check.outputs.scrape_reason }}"
        
        # Set target sites if specified
        TARGET_SITES_ARG=""
        if [ -n "${{ github.event.inputs.target_sites }}" ]; then
          TARGET_SITES_ARG="--sites ${{ github.event.inputs.target_sites }}"
          echo "Targeting specific sites: ${{ github.event.inputs.target_sites }}"
        fi
        
        # Run the scraper with appropriate flags
        FORCE_FLAG=""
        if [ "${{ github.event.inputs.force_full_scrape }}" = "true" ]; then
          FORCE_FLAG="--force"
        fi
        
        # Execute scraping with error handling
        if python main.py scrape $TARGET_SITES_ARG $FORCE_FLAG --output-dir ../data; then
          echo "scrape_success=true" >> $GITHUB_OUTPUT
          echo "✅ Scraping completed successfully"
          
          # Update last scrape timestamp
          date -u +"%Y-%m-%dT%H:%M:%SZ" > ../data/last_scrape.txt
          
          # Count new/updated RFPs
          NEW_RFPS=$(python -c "
import json
try:
    with open('../data/rfps.json', 'r') as f:
        rfps = json.load(f)
    print(len(rfps))
except:
    print(0)
" 2>/dev/null || echo "0")
          echo "rfp_count=$NEW_RFPS" >> $GITHUB_OUTPUT
          
        else
          echo "scrape_success=false" >> $GITHUB_OUTPUT
          echo "❌ Scraping failed"
          exit 1
        fi

    - name: Archive historical data
      if: steps.scrape_check.outputs.should_scrape == 'true' && steps.scrape.outputs.scrape_success == 'true'
      run: |
        # Create historical archive
        TIMESTAMP=$(date -u +"%Y%m%d_%H%M%S")
        mkdir -p data/history
        
        # Archive current data with timestamp
        if [ -f "data/rfps.json" ]; then
          cp data/rfps.json "data/history/rfps_$TIMESTAMP.json"
          echo "📁 Archived RFPs to data/history/rfps_$TIMESTAMP.json"
        fi
        
        # Keep only last 30 days of history to prevent repo bloat
        find data/history -name "rfps_*.json" -type f -mtime +30 -delete
        
        # Create summary statistics
        python -c "
import json
import os
from datetime import datetime

try:
    with open('data/rfps.json', 'r') as f:
        rfps = json.load(f)
    
    stats = {
        'timestamp': datetime.utcnow().isoformat() + 'Z',
        'total_rfps': len(rfps),
        'active_rfps': len([r for r in rfps if r.get('extracted_fields', {}).get('status', '').lower() == 'active']),
        'olympic_related': len([r for r in rfps if 'olympic' in r.get('categories', [])]),
        'surveillance_flagged': len([r for r in rfps if any(cat in ['surveillance', 'security', 'monitoring'] for cat in r.get('categories', []))]),
        'last_updated': datetime.utcnow().isoformat() + 'Z'
    }
    
    with open('data/stats.json', 'w') as f:
        json.dump(stats, f, indent=2)
    
    print(f'📊 Updated statistics: {stats[\"total_rfps\"]} total RFPs, {stats[\"olympic_related\"]} Olympic-related')
except Exception as e:
    print(f'Warning: Could not generate stats: {e}')
"

    - name: Commit and push changes
      if: steps.scrape_check.outputs.should_scrape == 'true' && steps.scrape.outputs.scrape_success == 'true'
      run: |
        # Check if there are any changes to commit
        if [ -n "$(git status --porcelain)" ]; then
          echo "📝 Changes detected, committing..."
          
          git add data/
          
          # Create detailed commit message
          COMMIT_MSG="🤖 Automated RFP update"
          if [ "${{ steps.scrape_check.outputs.scrape_reason }}" = "force_update" ]; then
            COMMIT_MSG="🚀 Force update: RFP data refresh"
          elif [ "${{ steps.scrape_check.outputs.scrape_reason }}" = "manual_trigger" ]; then
            COMMIT_MSG="👤 Manual trigger: RFP data update"
          fi
          
          COMMIT_MSG="$COMMIT_MSG

- Scraped at: $(date -u +"%Y-%m-%d %H:%M:%S UTC")
- Total RFPs: ${{ steps.scrape.outputs.rfp_count }}
- Trigger: ${{ steps.scrape_check.outputs.scrape_reason }}
- Workflow: ${{ github.workflow }} #${{ github.run_number }}"

          git commit -m "$COMMIT_MSG"
          git push
          
          echo "✅ Changes committed and pushed to repository"
        else
          echo "ℹ️ No changes detected - data is up to date"
        fi

    - name: Create scraping summary
      if: steps.scrape_check.outputs.should_scrape == 'true'
      run: |
        echo "## 🕸️ RFP Scraping Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "- **Status**: ${{ steps.scrape.outputs.scrape_success == 'true' && '✅ Success' || '❌ Failed' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Trigger**: ${{ steps.scrape_check.outputs.scrape_reason }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Total RFPs**: ${{ steps.scrape.outputs.rfp_count || 'Unknown' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Timestamp**: $(date -u +"%Y-%m-%d %H:%M:%S UTC")" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ steps.scrape_check.outputs.should_scrape }}" != "true" ]; then
          echo "**Scrape skipped** - too soon since last update or other constraint" >> $GITHUB_STEP_SUMMARY
        fi

  # Trigger deployment after successful scrape
  trigger-deployment:
    needs: scrape-rfps
    if: needs.scrape-rfps.result == 'success'
    runs-on: ubuntu-latest
    
    steps:
    - name: Trigger deployment workflow
      uses: actions/github-script@v7
      with:
        script: |
          github.rest.actions.createWorkflowDispatch({
            owner: context.repo.owner,
            repo: context.repo.repo,
            workflow_id: 'deploy.yml',
            ref: 'main'
          });