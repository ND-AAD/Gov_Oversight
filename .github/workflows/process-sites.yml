# GitHub Actions Workflow: Process New Sites and Run Scraping
# 
# Triggered when sites.json is updated (by Vercel API direct file updates)
# Automatically runs scraping for newly added sites

name: Process Sites and Scraping

on:
  push:
    paths:
      - 'data/sites.json'
  schedule:
    # Regular scraping every 6 hours
    - cron: '0 */6 * * *'
  workflow_dispatch:
    # Manual trigger for immediate scraping

jobs:
  detect-changes:
    runs-on: ubuntu-latest
    if: github.event_name == 'push'
    outputs:
      new-sites: ${{ steps.check.outputs.new-sites }}
      has-new-sites: ${{ steps.check.outputs.has-new-sites }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 2
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Check for new sites
        id: check
        run: |
          echo "Checking for new sites in sites.json..."
          
          # Get the current sites.json
          CURRENT_SITES=$(jq -r '.sites[].id' data/sites.json | wc -l)
          
          # Get the previous sites.json (if it exists)
          if git show HEAD~1:data/sites.json > /dev/null 2>&1; then
            PREVIOUS_SITES=$(git show HEAD~1:data/sites.json | jq -r '.sites[].id' | wc -l)
          else
            PREVIOUS_SITES=0
          fi
          
          if [ "$CURRENT_SITES" -gt "$PREVIOUS_SITES" ]; then
            echo "has-new-sites=true" >> $GITHUB_OUTPUT
            echo "New sites detected: $CURRENT_SITES sites (was $PREVIOUS_SITES)"
            
            # Get the new site IDs
            if [ "$PREVIOUS_SITES" -gt 0 ]; then
              git show HEAD~1:data/sites.json | jq -r '.sites[].id' > /tmp/previous_sites.txt
              jq -r '.sites[].id' data/sites.json > /tmp/current_sites.txt
              NEW_SITES=$(comm -23 /tmp/current_sites.txt /tmp/previous_sites.txt | tr '\n' ',' | sed 's/,$//')
            else
              NEW_SITES=$(jq -r '.sites[].id' data/sites.json | tr '\n' ',' | sed 's/,$//')
            fi
            
            echo "new-sites=$NEW_SITES" >> $GITHUB_OUTPUT
            echo "New site IDs: $NEW_SITES"
          else
            echo "has-new-sites=false" >> $GITHUB_OUTPUT
            echo "No new sites detected"
          fi

  scrape-rfps:
    runs-on: ubuntu-latest
    needs: [detect-changes]
    if: always() && (github.event_name == 'schedule' || github.event_name == 'workflow_dispatch' || needs.detect-changes.outputs.has-new-sites == 'true')
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install backend dependencies
        run: |
          cd backend
          pip install -r requirements.txt

      - name: Install Playwright browsers
        run: |
          cd backend
          playwright install chromium

      - name: Run RFP scraping
        id: scrape
        run: |
          cd backend
          echo "ðŸš€ Starting RFP scraping..."
          
          # Run the scraper
          python main.py scrape --output ../data/rfps.json 2>&1 | tee scrape_output.log
          
          # Get statistics
          SITES_COUNT=$(python main.py list-sites 2>/dev/null | grep -c "Site:" || echo "0")
          RFPS_COUNT=$(jq -r '.rfps | length' ../data/rfps.json 2>/dev/null || echo "0")
          
          echo "sites-count=$SITES_COUNT" >> $GITHUB_OUTPUT
          echo "rfps-count=$RFPS_COUNT" >> $GITHUB_OUTPUT
          
          echo "âœ… Scraping completed: $RFPS_COUNT RFPs from $SITES_COUNT sites"

      - name: Update statistics
        run: |
          cd backend
          echo "ðŸ“Š Updating statistics..."
          python -c "
          import json
          from datetime import datetime
          
          # Update stats.json
          stats = {
              'last_scrape': datetime.now().isoformat(),
              'total_sites': ${{ steps.scrape.outputs.sites-count }},
              'total_rfps': ${{ steps.scrape.outputs.rfps-count }},
              'last_updated': datetime.now().isoformat()
          }
          
          with open('../data/stats.json', 'w') as f:
              json.dump(stats, f, indent=2)
          
          print('âœ… Statistics updated')
          "

      - name: Commit updated data
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          # Check if there are changes to commit
          if git diff --quiet data/; then
            echo "No changes to commit"
          else
            git add data/
            
            if [ "${{ needs.detect-changes.outputs.has-new-sites }}" == "true" ]; then
              COMMIT_MSG="ðŸš€ Scrape RFPs for newly added sites: ${{ needs.detect-changes.outputs.new-sites }}

              Found ${{ steps.scrape.outputs.rfps-count }} RFPs from ${{ steps.scrape.outputs.sites-count }} sites
              
              ðŸ¤– Generated with GitHub Actions"
            else
              COMMIT_MSG="ðŸ”„ Scheduled RFP scraping update

              Found ${{ steps.scrape.outputs.rfps-count }} RFPs from ${{ steps.scrape.outputs.sites-count }} sites
              
              ðŸ¤– Generated with GitHub Actions"
            fi
            
            git commit -m "$COMMIT_MSG"
            git push
            echo "âœ… Successfully committed updated data"
          fi

      - name: Summary
        run: |
          echo "ðŸŽ¯ RFP scraping complete"
          echo "Sites processed: ${{ steps.scrape.outputs.sites-count }}"
          echo "RFPs found: ${{ steps.scrape.outputs.rfps-count }}"
          if [ "${{ needs.detect-changes.outputs.has-new-sites }}" == "true" ]; then
            echo "New sites added: ${{ needs.detect-changes.outputs.new-sites }}"
          fi